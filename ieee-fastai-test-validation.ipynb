{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.60\n",
      "/kaggle/input/ieee-reduce-mem/__results__.html\n",
      "/kaggle/input/ieee-reduce-mem/custom.css\n",
      "/kaggle/input/ieee-reduce-mem/test.pkl\n",
      "/kaggle/input/ieee-reduce-mem/__notebook__.ipynb\n",
      "/kaggle/input/ieee-reduce-mem/__output__.json\n",
      "/kaggle/input/ieee-reduce-mem/train.pkl\n",
      "/kaggle/input/ieee-reduce-mem/__results___files/__results___28_2.png\n",
      "/kaggle/input/ieee-fastai-wd/__results__.html\n",
      "/kaggle/input/ieee-fastai-wd/400-100-model_1.pkl\n",
      "/kaggle/input/ieee-fastai-wd/custom.css\n",
      "/kaggle/input/ieee-fastai-wd/__notebook__.ipynb\n",
      "/kaggle/input/ieee-fastai-wd/__output__.json\n",
      "/kaggle/input/ieee-fastai-wd/__results___files/__results___12_2.png\n",
      "/kaggle/input/ieee-fastai-wd/__results___files/__results___10_2.png\n",
      "/kaggle/input/ieee-fastai-wd/models/tmp.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import fastai\n",
    "from fastai import metrics\n",
    "from fastai.tabular import FillMissing, Categorify, Normalize, DatasetType\n",
    "from fastai.tabular import TabularDataBunch, tabular_learner, TabularList, load_learner\n",
    "print(fastai.__version__)\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "INPUT = pathlib.Path('/kaggle/input')\n",
    "MODELS = INPUT / 'ieee-fastai-wd'\n",
    "\n",
    "NROWS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    test = pd.read_pickle(INPUT / 'ieee-reduce-mem' / 'test.pkl')\n",
    "    if NROWS:\n",
    "        test = test[:NROWS]\n",
    "    return test\n",
    "\n",
    "def test_pipeline(test):\n",
    "    print(f\"==test_pipeline==\")\n",
    "    test = test.reset_index(drop=True)\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "    cat_names, cont_names = get_cat_names(test)\n",
    "    data = TabularList.from_df(test, cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "    return data\n",
    "\n",
    "def get_cat_names(df):\n",
    "    \"\"\"Get a list of all category column names\"\"\"\n",
    "    print(f\"\\n==get_cat_names==\")\n",
    "    cards = [f\"card{i}\" for i in range(1, 7)]\n",
    "    matches = [f\"M{i}\" for i in range(1, 10)]\n",
    "    trx_cats = [\"ProductCD\", \"addr1\", \"addr2\", \"P_emaildomain\", \"R_emaildomain\", *cards, *matches]\n",
    "    ids = [f\"id_{i}\" for i in range(12, 39)]\n",
    "    id_cats = [\"DeviceType\", \"DeviceInfo\", *ids]\n",
    "    cat_names = trx_cats + id_cats\n",
    "    cat_names = [x for x in cat_names if x in df.columns]\n",
    "    cont_names = [x for x in df.columns if x not in cat_names]\n",
    "\n",
    "    excluded_columns = ['TransactionID', 'TransactionDT', 'isFraud']\n",
    "    for col in excluded_columns:\n",
    "        for col_list in [cat_names, cont_names]:\n",
    "            if col in col_list:\n",
    "                col_list.remove(col)\n",
    "    print(f\"cat_names: {len(cat_names)} {cat_names[:10]}...\")\n",
    "    print(f\"cont_names: {len(cont_names)} {cont_names[:10]}...\")\n",
    "    return cat_names, cont_names\n",
    "\n",
    "def get_test_learner():\n",
    "    test = load_test_data()\n",
    "    test = test_pipeline(test)\n",
    "    learn = load_learner(MODELS,'400-100-model_1.pkl', test=test)\n",
    "    return learn\n",
    "\n",
    "def save_test_set_preds(preds, indexes):\n",
    "    df1 = pd.DataFrame(\n",
    "        {'TransactionID': indexes,\n",
    "         'isFraud': [float(x) for x in preds[0]]},\n",
    "    )\n",
    "    df1.to_csv('submission1.csv', index=False)\n",
    "    df2 = pd.DataFrame(\n",
    "        {'TransactionID': indexes,\n",
    "         'isFraud': [float(x) for x in preds[1]]},\n",
    "    )\n",
    "    df2.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST AGAINST KNOWN LABELS\n",
    "* Lets save validation test output for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==test_pipeline==\n",
      "\n",
      "==get_cat_names==\n",
      "cat_names: 49 ['ProductCD', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'card1', 'card2', 'card3', 'card4', 'card5']...\n",
      "cont_names: 382 ['TransactionAmt', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = get_test_learner()\n",
    "preds, y = learn.get_preds(ds_type=DatasetType.Test)\n",
    "preds = list(zip(*preds))\n",
    "indexes = learn.data.test_ds.x.inner_df['TransactionID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_test_set_preds(preds, indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Thoughts\n",
    "The score against the validation set was 0.93. When tested against the public test set, it scored significantly worse (~0.87). I don't think this is due to overfitting. Most of the other kernels experienced a similar deterioration in score and it's likely because the test dataset and the train dataset were split by time. There were other significant differences between the train and test datasets as well, such as the proportion of missing of null values in each column. Future research should focus on how to handle these differences. Figuring out which features to exclude/manipulate will likely improve performance more than tweaking the model hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
